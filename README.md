# ğŸš€ Mini LLM Post-Training Pipeline (SFT + DPO + Eval + Experiment Logs)

*A minimal, production-inspired post-training workflow for aligning open-source LLMs using Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), structured experiment logging, and offline A/B-style evaluation.*

This project implements a **fully reproducible, end-to-end post-training pipeline** similar to what modern alignment teams use in industry (Inflection AI, Anthropic, OpenAI). It provides:

- ğŸ”§ **Config-driven SFT and DPO training** (PyTorch + HuggingFace + TRL + LoRA)  
- ğŸ“š **Dataset curation** for instruction-tuning & preference pairs  
- ğŸ“Š **Batch evaluation + A/B comparison** across Base / SFT / DPO models  
- ğŸ§ª **Keyword-based scoring + per-prompt win-rate metrics**  
- ğŸ“ **Structured experiment logging** for reproducibility  
- âš™ï¸ **Modular design** to extend with RLHF, reward models, ORPO, or larger architectures  
- ğŸ§µ **TinyLlama 1.1B** as default lightweight backbone for fast local experiments  

The goal is to provide a **clean, understandable, and realistic** implementation of a modern LLM post-training workflowâ€”small enough to run locally, yet architected like a real industrial system.

---

## ğŸ¯ Why This Project Exists

Most open-source LLM tutorials show only a single training script or a small notebook.  
But real alignment work requires **pipelines, evaluation loops, experiment logs, and reproducibility**.

This project aims to bridge that gap:

> **â€œA minimal pipeline that mirrors real-world LLM post-training systems, but small enough for one person to run and understand fully.â€**

It is especially suited for:

- Candidates preparing for **Applied ML / LLM Infra / Model Optimization** interviews  
- Researchers wanting a clean baseline for SFT â†’ DPO â†’ Eval  
- Students learning how RLHF-era post-training systems are structured  
- Engineers building their first alignment pipeline  

---

## ğŸ§© Features at a Glance

| Component | Description |
|----------|-------------|
| **SFT Training** | LoRA-based instruction tuning with config-driven training |
| **DPO Training** | Preference optimization with TRL (policy + reference model) |
| **Dataset Curation** | JSONL instruction and preference pair format |
| **Batch Evaluation** | Compare Base / SFT / DPO outputs at scale |
| **Metrics** | Keyword hit-rate & win-rate analysis |
| **A/B Testing** | Side-by-side answer comparison per prompt |
| **Experiment Logging** | Auto-save JSON logs for every run |
| **Reproducibility** | Seed control + config files + full run history |

---

## ğŸ› ï¸ Tech Stack

- **PyTorch** â€” Core training execution  
- **HuggingFace Transformers** â€” Model & tokenizer  
- **TRL (Transformer Reinforcement Learning)** â€” DPO implementation  
- **PEFT / LoRA** â€” Parameter-efficient tuning  
- **YAML configs** â€” Full pipeline configurability  
- **JSONL datasets** â€” Easy dataset curation & extension  

The entire project is intentionally dependency-light and easy to run locally.

---

## ğŸ“¦ Project Structure

```text
mini-llm-alignment-pipeline/
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ sft_config.yaml
â”‚   â””â”€â”€ dpo_config.yaml
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sft_examples.jsonl
â”‚   â”œâ”€â”€ dpo_pairs.jsonl
â”‚   â””â”€â”€ eval_prompts.jsonl
â”‚
â”œâ”€â”€ aligner/
â”‚   â”œâ”€â”€ data.py
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ eval.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_sft.py
â”‚   â”œâ”€â”€ train_dpo.py
â”‚   â””â”€â”€ eval_batch.py
â”‚
â”œâ”€â”€ experiments/        â† auto-generated logs
â””â”€â”€ outputs/            â† trained model checkpoints
```

## âš¡ Quickstart

### 1. Install Requirements

```bash
# Option 1
pip3 install -r requirements.txt

# Option 2
pip install -r requirements.txt
```
### 2. Run SFT Training
```bash
python3 -m scripts.train_sft --config configs/sft_config.yaml
```
### 3. Run DPO Training
```bash
python3 -m scripts.train_dpo --config configs/dpo_config.yaml
```
### 4. Evaluate all models
```bash
python3 -m scripts.eval_batch \
  --base TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
  --sft outputs/sft \
  --dpo outputs/dpo
```
### 5. Inspect Experiments logs
```bash
ls experiments/
cat experiments/sft-*.json
```
## ğŸ“Š Evaluation & A/B Testing

This project includes a lightweight but practical offline evaluation framework for comparing the base, SFT, and DPO models across domain-specific prompts in probability, Markov chains, and time-series analysis.

The evaluation pipeline consists of: 
- Batch generation for all models on the same prompt set
- Keyword-based scoring (checks if key statistical ideas appear in the answer)
- Per-prompt win-rate analysis
- tructured logs saved to experiments/eval_batch_results.jsonl